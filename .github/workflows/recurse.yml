name: Recursive Enumeration Worker (3-Layer Parallel)

on:
  workflow_dispatch:
    inputs:
      primary_github_server_url:
        description: 'The server URL of the primary GitHub instance.'
        required: true
      primary_repo_owner:
        description: 'The owner of the primary repository.'
        required: true
      primary_repo_name:
        description: 'The name of the primary repository.'
        required: true
      primary_run_id:
        description: 'The run ID of the primary workflow.'
        required: true
      chunk_package_artifact_name:
        description: 'The name of the artifact package containing all seed chunks.'
        required: true
      secondary_matrix_json:
        description: 'The JSON string representing the matrix of chunks assigned to this worker.'
        required: true

permissions:
  contents: write
  actions: read

jobs:
  process_assigned_chunks_worker:
    name: Process Assigned Chunks (Worker)
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ secrets.GHCR_USER }}
        password: ${{ secrets.GHCR_TOKEN }}
    strategy:
      fail-fast: false
      max-parallel: 20
      matrix:
        pair: ${{ fromJson(github.event.inputs.secondary_matrix_json && github.event.inputs.secondary_matrix_json || '[]') }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.23'

      - name: Cache Go modules & binaries
        uses: actions/cache@v3
        with:
          path: |
            $HOME/go/pkg/mod
            ~/.cache/go-build
            $HOME/go/bin
          key: ${{ runner.os }}-go-cache-${{ github.ref_name }}
          restore-keys: |
            ${{ runner.os }}-go-cache-

      - name: Download Full Chunks Package from Primary Account
        env:
          GH_TOKEN_PRIMARY_ACCOUNT_READ: ${{ secrets.PAT_FOR_PRIMARY_ACCOUNT_ARTIFACTS_READ }}
          PRIMARY_REPO_OWNER: ${{ github.event.inputs.primary_repo_owner }}
          PRIMARY_REPO_NAME: ${{ github.event.inputs.primary_repo_name }}
          PRIMARY_RUN_ID: ${{ github.event.inputs.primary_run_id }}
          ARTIFACT_NAME_FROM_PRIMARY: ${{ github.event.inputs.chunk_package_artifact_name }}
        shell: bash
        run: |
          echo "WORKER: Downloading artifact '$ARTIFACT_NAME_FROM_PRIMARY' from $PRIMARY_REPO_OWNER/$PRIMARY_REPO_NAME"
          if ! command -v gh &> /dev/null; then
            apt-get update -qy && apt-get install -qy curl && \
            curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg && \
            chmod go+r /usr/share/keyrings/githubcli-archive-keyring.gpg && \
            echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | tee /etc/apt/sources.list.d/github-cli.list > /dev/null && \
            apt-get update -qy && apt-get install -qy gh || { echo "ERROR: gh CLI installation failed.";  }
          fi
          echo "$GH_TOKEN_PRIMARY_ACCOUNT_READ" | gh auth login --with-token
          gh run download "$PRIMARY_RUN_ID" -R "$PRIMARY_REPO_OWNER/$PRIMARY_REPO_NAME" -n "$ARTIFACT_NAME_FROM_PRIMARY" --dir .
          PACKAGE_FILENAME="$ARTIFACT_NAME_FROM_PRIMARY.tar.gz"
          if [ -f "$PACKAGE_FILENAME" ]; then
            echo "Downloaded '$PACKAGE_FILENAME'. Extracting..."
            tar -xzvf "$PACKAGE_FILENAME"
          else
            echo "::error:: Failed to download or find artifact tarball '$PACKAGE_FILENAME'"
            exit 0
          fi

      - name: Run Findomain with GNU Parallel on Chunk
        id: run_parallel
        shell: bash
        run: |
        
          CHUNK_FILE_PATH=${{ matrix.pair.chunk }}
          echo "Processing chunk '$CHUNK_FILE_PATH'..."
          if [ ! -f "$CHUNK_FILE_PATH" ]; then
            echo "ERROR: Chunk file '$CHUNK_FILE_PATH' not found!"
            exit 0
          fi
          SUB_CHUNK_DIR="sub-chunks"
          RAW_RESULTS_FILE="raw_subdomain_results.txt"
          export SUB_CHUNK_DIR
          rm -rf "$SUB_CHUNK_DIR"
          mkdir -p "$SUB_CHUNK_DIR" "${SUB_CHUNK_DIR}/raw_results"
          TOTAL_LINES=$(wc -l < "$CHUNK_FILE_PATH")
          LINES_PER_SUB_CHUNK=$(awk -v t="$TOTAL_LINES" 'BEGIN { print int((t+4)/5) }')
          echo "WORKER: Main chunk has $TOTAL_LINES lines. Splitting into 5 sub-chunks of up to $LINES_PER_SUB_CHUNK lines each."
          split -l "$LINES_PER_SUB_CHUNK" "$CHUNK_FILE_PATH" "${SUB_CHUNK_DIR}/sub_chunk_"
          parallel -j 5 'findomain --file "{}" --external-subdomains --quiet --unique-output "${SUB_CHUNK_DIR}/raw_results/result_{#}.txt"' ::: ${SUB_CHUNK_DIR}/sub_chunk_*
          cat "$SUB_CHUNK_DIR/raw_results"/result_*.txt | sort -u > "$RAW_RESULTS_FILE"
          echo "✅ All done. Consolidated into $RAW_RESULTS_FILE with $(wc -l < "$RAW_RESULTS_FILE") subdomains"

      - name: Install Tools
        run: |
          if ! command -v smap >/dev/null; then go install -v github.com/s0md3v/smap/cmd/smap@latest; fi
          if ! command -v inscope >/dev/null; then go install -v github.com/tomnomnom/hacks/inscope@latest; fi
          if ! command -v anew >/dev/null; then go install -v github.com/tomnomnom/anew@latest; fi
          if ! command -v cut-cdn >/dev/null; then go install github.com/ImAyrix/cut-cdn@latest; fi
          if ! command -v naabu >/dev/null; then go install -v github.com/projectdiscovery/naabu/v2/cmd/naabu@latest; fi
          pip3 install --no-cache-dir ipaddress
          echo "$HOME/go/bin" >> $GITHUB_PATH

      - name: Resolve Discovered Subdomains with PureDNS
        id: run_puredns
        shell: bash
        run: |
                RAW_RESULTS_FILE="raw_subdomain_results.txt"
                PUREDNS_FILE="puredns_file.txt"
                RESOLVED_FILE="puredns_resolved.txt"
                TMP_CLEANMASSDNS=$(mktemp)
                MASSDNS="massdns.txt"
                #WILDCARD_FILE="wildcard_sub.txt"
                MASSDNS_FILE="massdns_file.txt"
                
                if [ ! -s "$RAW_RESULTS_FILE" ]; then
                        echo "INFO: Raw results file is empty. Nothing to resolve."
                        touch "$PUREDNS_FILE" # Create an empty file to prevent downstream errors
                        exit 0
                fi
                
                wget -q0 resolvers.txt https://raw.githubusercontent.com/rix4uni/resolvers/refs/heads/main/resolvers.txt
                
                wget -qO resolvers-trusted.txt https://raw.githubusercontent.com/and0x00/resolvers.txt/refs/heads/main/resolvers.txt
                          
                echo "INFO: Resolving subdomains using puredns..."
                               
                puredns resolve "$RAW_RESULTS_FILE" \
                  -r resolvers.txt \
                  --resolvers-trusted resolvers-trusted.txt \
                  --rate-limit 3000 \
                  --wildcard-tests 300 \
                  --wildcard-batch 100000 \
                  --rate-limit-trusted 2000 \
                  --write "$PUREDNS_FILE" \
                  --write-massdns "$MASSDNS" \
                  --quiet >/dev/null 2>&1
                                     
                echo "✅ PureDNS resolution complete. $(wc -l < "$PUREDNS_FILE") subdomains were successfully resolved."
                echo "✅ Downloading .scope file from recon-automation repo"
                wget -qO .scope https://raw.githubusercontent.com/Pcoder7/recon-automation/refs/heads/main/.scope 
                echo  " $(wc -l < .scope) lines of scope file is downloaded " 
                
                cat "$PUREDNS_FILE" | inscope -s .scope > "$RESOLVED_FILE" || true
                # echo "✅ PureDNS resolution complete. $(wc -l < "$WILDCARD_FILE") Wildcard subdomains found."
                awk 'NF { sub(/\.$/,"",$1); print }' "$MASSDNS" > "$TMP_CLEANMASSDNS"
                awk ' \
                {gsub(/\r$/,"");sub(/^[ \t]+/,"");sub(/[ \t]+$/,"")} \
                FNR==NR{if($0)patterns[++c]=$0;next} \
                !setup{regex="";for(i=1;i<=c;i++){regex=regex (i>1?"|":"") "("patterns[i]")"};if(regex=="")regex="^\b$";setup=1} \
                $2=="A" && $1~regex \
                ' .scope "$TMP_CLEANMASSDNS" | anew -q "$MASSDNS_FILE"
                
                echo "✅ PureDNS resolution complete. $(wc -l < "$MASSDNS_FILE")  massdns found."     
                rm -f "$TMP_CLEANMASSDNS" "$MASSDNS" "$PUREDNS_FILE"        
     
      - name: Sort Resolved Results into Root Domain Folders
        shell: bash
        run: |
                RESOLVED_FILE="puredns_resolved.txt"
                MASSDNS_FILE="massdns_file.txt"
                mkdir -p results
                # -------------------------
                # Option B: in-memory awk filter
                # Keep only massdns lines whose hostname (first field) exists in RESOLVED_FILE
                # -------------------------
                # If massdns file doesn't exist or is empty -> nothing to do
                if [ ! -s "$MASSDNS_FILE" ]; then
                        echo "INFO: No massdns file ($MASSDNS_FILE) present or empty; skipping filter."
                else
                        # If resolved file empty -> produce empty MASSDNS_FILE (no extraneous junk)
                        if [ ! -s "$RESOLVED_FILE" ]; then
                                echo "INFO: Resolved file is empty -> clearing $MASSDNS_FILE per policy."
                                : > "$MASSDNS_FILE"
                        else
                                echo "INFO: Filtering $MASSDNS_FILE using allowlist from $RESOLVED_FILE (in-memory awk)."
                                TMP_FILTERED=$(mktemp)
                                awk '
                                # First pass: read allowlist (RESOLVED_FILE) and use first field as hostname
                                FNR==NR {
                                        gsub(/\r/,""); sub(/^[ \t]+|[ \t]+$/,"");
                                        sub(/\.$/,"");
                                        host=$1;              # <-- use first field (hostname) not entire line
                                        host=tolower(host);
                                        if(length(host)) allowed[host]=1;
                                        next
                                }
                                # Second pass: process massdns lines (MASSDNS_FILE)
                                NF {
                                        host=$1; sub(/\.$/,"",host);
                                        if(allowed[tolower(host)]) print $0;
                                }' "$RESOLVED_FILE" "$MASSDNS_FILE" > "$TMP_FILTERED"
                                # atomically replace original massdns file with filtered one
                                mv "$TMP_FILTERED" "$MASSDNS_FILE"
                                echo "INFO: Filtered massdns count: $(wc -l < "$MASSDNS_FILE" || true)"
                        fi
                fi
                # -------------------------
                # Build combined list (same as before)
                # -------------------------
                # Create a combined list of all subdomains from both files to find all possible root domains
                cat "$RESOLVED_FILE" 2>/dev/null > combined_subdomains.txt
                awk '{print $1}' "$MASSDNS_FILE" 2>/dev/null >> combined_subdomains.txt
                if [ ! -s "combined_subdomains.txt" ]; then echo "INFO: No resolvable data in this chunk."; exit 0; fi
                # Extract unique root domains from the combined list
                dsieve -if "combined_subdomains.txt" -f 2 | sort -u > temp_root_domains.txt
                while read -r parent; do
                        if [ -z "$parent" ]; then continue; fi; mkdir -p "results/$parent"
                        # Logic for puredns results (only if the source file has content)
                        if [ -s "$RESOLVED_FILE" ]; then
                                grep -E "(^|\\.)${parent//./\\.}(\$)" --color=never "$RESOLVED_FILE" | anew -q "results/$parent/puredns_results.txt" || true
                        fi
                        # Logic for massdns file (only if the source file has content)
                        if [ -s "$MASSDNS_FILE" ]; then
                                grep -E "(^|\\.)${parent//./\\.}(\s|\$)" --color=never "$MASSDNS_FILE" | anew -q "results/$parent/massdns_file.txt" || true
                        fi
                done < temp_root_domains.txt
                
      - name: Compute SAFE_CHUNK (no slashes)
        run: |
          SAFE_CHUNK="${{ matrix.pair.chunk }}"
          SAFE_CHUNK="$(echo "$SAFE_CHUNK" | tr '/' '_')"
          echo "SAFE_CHUNK=$SAFE_CHUNK" >> $GITHUB_ENV

      - name: Upload Secondary Account Results
        uses: actions/upload-artifact@v4
        with:
          name: recon-results-secondary-${{ env.SAFE_CHUNK }}
          path: results/
          retention-days: 1

  merge_results:
    name: Merge All Secondary Worker Results
    runs-on: ubuntu-latest
    needs: process_assigned_chunks_worker
    if: always()
    outputs:
      has_results: ${{ steps.check_artifacts.outputs.found }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Download all result artifacts from this worker account
        uses: actions/download-artifact@v4
        with:
          pattern: 'recon-results-secondary-*' # Downloads only this worker's artifacts
          path: temp-aggregated-results
          merge-multiple: true

      - name: Check if artifacts were downloaded
        id: check_artifacts
        shell: bash
        run: |
          if [ -d "temp-aggregated-results" ] && [ -n "$(ls -A temp-aggregated-results)" ]; then
            echo "-> Artifacts found. Proceeding with merge."
            echo "found=true" >> $GITHUB_OUTPUT
          else
            echo "-> No artifacts found to merge. Skipping the rest of this job."
            echo "found=false" >> $GITHUB_OUTPUT
          fi
          
      - name: Consolidate all results into root domain folders
        id: consolidate
        run: |
          mkdir -p final_results
          
          # Aggregate puredns_results.txt files
          
          find temp-aggregated-results -type f -name "puredns_results.txt" | while read -r f; do D=$(basename "$(dirname "$f")"); mkdir -p "final_results/$D"; cat "$f" >> "final_results/$D/puredns_results.txt"; done
         
          # Aggregate massdns_file.txt files
          
          find temp-aggregated-results -type f -name "massdns_file.txt" | while read -r f; do D=$(basename "$(dirname "$f")"); mkdir -p "final_results/$D"; cat "$f" >> "final_results/$D/massdns_file.txt"; done
          
          # De-duplicate all aggregated files
          find final_results -type f -name "*.txt" -exec sort -u -o {} {} \;
          
         
          # Final guard clause to check if the process resulted in any actual data.
         
          if [ -z "$(ls -A final_results)" ]; then
            echo "::warning:: Result artifacts were downloaded, but they contained no valid data to merge."
            echo "has_results=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "has_results=true" >> $GITHUB_OUTPUT
          echo "✅ Successfully consolidated results from all accounts."
          ls -R final_results

      - name: Upload Final Consolidated Secondary Artifact
        if: steps.consolidate.outputs.has_results == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: consolidated-secondary-recon-results
          path: final_results/
          retention-days: 1

  commit_all_results:
    name: Commit Secondary Worker Results
    needs: merge_results
    if: always() && needs.merge_results.outputs.has_results == 'true'
    runs-on: ubuntu-latest

    steps:
      - name: Download the consolidated secondary results artifact
        uses: actions/download-artifact@v4
        with:
          name: consolidated-secondary-recon-results
          path: final_results

      - name: Organize and Push to store-recon
        shell: bash
        env:
          STORE_RECON_PAT: ${{ secrets.PAT_FOR_SECONDARY_ACCOUNT_REPO }}
          ACCOUNT2_USERNAME: ${{ secrets.ACCOUNT2_REPO_OWNER }}
          STORE: ${{ secrets.STORE }}
          CORRELATION_ID: ${{ github.event.inputs.primary_run_id }}
        run: |
          RESULTS_DIR="${GITHUB_WORKSPACE}/final_results"
          
          if [ ! -d "$RESULTS_DIR" ] || [ -z "$(ls -A "$RESULTS_DIR")" ]; then
            echo "::warning:: Results directory is empty or does not exist. Nothing to commit."
            exit 0
          fi
          
          echo "Cloning ${STORE} to commit results from secondary worker..."
          git config --global user.name "Secondary Worker Bot"
          git config --global user.email "actions-bot@users.noreply.github.com"
          
          TMP_DIR="$(mktemp -d)"
          if ! git clone "https://x-access-token:${STORE_RECON_PAT}@github.com/${ACCOUNT2_USERNAME}/${STORE}.git" "$TMP_DIR"; then
            echo "::error:: Failed to clone the repository. Aborting the commit process."
            exit 0
          fi
          cd "$TMP_DIR"
          
          # --- Reusable function to merge artifact data ---
          run_merge() {
            echo "Merging new secondary results into the repository..."
            for domain_dir in "${RESULTS_DIR}"/*; do
              if [ ! -d "$domain_dir" ]; then continue; fi
              domain_name=$(basename "$domain_dir")
              dest_repo_dir="results/$domain_name"
              mkdir -p "$dest_repo_dir"
              
              # --- MERGE BLOCK 1  ---
              # Logic to process puredns results
              source_puredns_file="$domain_dir/puredns_results.txt"
              dest_all_subs_file="$dest_repo_dir/all_subdomains.txt"
              # FIX: Correctly check if the puredns file exists and is not empty
              if [ -s "$source_puredns_file" ]; then
                # FIX: Correctly sanitize the puredns file, not the ports file
                <"$source_puredns_file" tr -d '\0' \
                  | grep '[[:alnum:]]' \
                  | sed -e 's/^[[:space:]]*//' -e 's/[[:space:]]*$//' \
                  | sed -r "s/\x1B\[([0-9]{1,2}(;[0-9]{1,2})?)?[m|K]//g" \
                  > "$source_puredns_file.tmp" && mv "$source_puredns_file.tmp" "$source_puredns_file"

                echo "  -> Merging puredns results into '$dest_all_subs_file'"
                temp_merged_file_1=$(mktemp)
                if [ -f "$dest_all_subs_file" ]; then
                  cat "$source_puredns_file" "$dest_all_subs_file" | sort -u > "$temp_merged_file_1"
                else
                  sort -u "$source_puredns_file" > "$temp_merged_file_1"
                fi
                mv "$temp_merged_file_1" "$dest_all_subs_file"
              fi
              
              # FIX: Added MERGE BLOCK 2 for massdns with full sanitization
              
              source_massdns_file="$domain_dir/massdns_file.txt"; 
              dest_massdns_file="$dest_repo_dir/massdns.txt"
              if [ -s "$source_massdns_file" ]; then
                <"$source_massdns_file" tr -d '\0'|grep '[[:alnum:]]'|sed -e 's/^[[:space:]]*//' -e 's/[[:space:]]*$//'|sed -r "s/\x1B\[([0-9]{1,2}(;[0-9]{1,2})?)?[m|K]//g" > tmp.txt && mv tmp.txt "$source_massdns_file"
                cat "$source_massdns_file" "$dest_massdns_file" 2>/dev/null | sort -u > tmp.txt && mv tmp.txt "$dest_massdns_file"
              fi
            done         
            
            # Stage all changes
            git add results/
          }

         

          # 1. Perform the initial merge and check for changes
          run_merge
          if git diff --cached --quiet; then
            echo "No new unique data to commit from secondary worker."
            exit 0
          fi
          
          # 2. Commit the changes locally
          echo "Committing changes locally..."
          git commit -m "feat: Add new assets from Secondary Worker scan from Correlation ID: ${CORRELATION_ID}"
          
          # 3. Loop to sync and push the commit, with a robust retry mechanism
          MAX_ATTEMPTS=10
          for (( i=1; i<=MAX_ATTEMPTS; i++ )); do
            echo "[Attempt $i/$MAX_ATTEMPTS] Pushing changes from secondary worker..."
            
            # Optimistic push first
            if git push -v origin main; then
              echo "✅ Successfully pushed new secondary results to ${STORE} on attempt $i."
              exit 0 
            fi
            
            echo "::warning:: Push failed on attempt $i. Fetching latest changes from remote and re-applying local changes."
            
            # If push failed, fetch the latest state from the remote
            git fetch origin main
            if [ $? -ne 0 ]; then
                echo "::error:: Git fetch failed. Cannot safely retry."
                sleep $(( 5 * i ))
                continue # Try again
            fi
            
            # Reset local state to match remote, discarding the old local commit
            git reset --hard origin/main
            
            # Re-run the merge logic on top of the fresh, updated branch
            echo "Re-applying merge logic on top of the updated main branch..."
            run_merge
            
            # Check if there are still changes to commit after the re-merge
            if git diff --cached --quiet; then
              echo "No net new changes to commit after syncing with remote. Another run may have already pushed these results."
              exit 0
            fi
            
            # Re-commit the newly calculated changes
            echo "Re-committing changes for retry attempt..."
            git commit -m "feat: Add new assets from Secondary Worker scan from Correlation ID: ${CORRELATION_ID} (retry)"

            sleep $(( 5 * i ))
          done

          echo "::error:: All $MAX_ATTEMPTS push attempts failed. The job will pass but the commit was NOT pushed."
          exit 0 
          
